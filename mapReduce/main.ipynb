{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating spark session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from the source database\n",
    "df = spark.read.jdbc(\n",
    "    url        = f'jdbc:mysql://source-db:{config.MySQL.port}/{config.MySQL.database}',\n",
    "    table      = config.MySQL.table,\n",
    "    properties = {\n",
    "        'user'    : config.MySQL.user,\n",
    "        'password': config.MySQL.password,\n",
    "        'driver'  : 'com.mysql.cj.jdbc.Driver'\n",
    "    }\n",
    ")\n",
    "\n",
    "# df = df.limit(1_000_000)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observing schema of the data\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing the dataset\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only the useful columns\n",
    "df = df.select(['createdAt', 'userName', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the text of tweets\n",
    "df = df.withColumn(\n",
    "    # replacing every username with '@user'\n",
    "    colName= 'text',\n",
    "    col    = pyspark.sql.functions.regexp_replace(\n",
    "        pyspark.sql.functions.col('text'), \n",
    "        pattern    = r'@\\S*',\n",
    "        replacement= '@user'\n",
    "    )\n",
    ").withColumn(\n",
    "    # replacing http/https urls with 'http'\n",
    "    colName = 'text',\n",
    "    col     = pyspark.sql.functions.regexp_replace(\n",
    "        pyspark.sql.functions.col('text'), \n",
    "        pattern    = r'http\\S*',\n",
    "        replacement= 'http'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( df.write\n",
    "    .format('mongodb')\n",
    "    .option('database'      , f'{config.MongoDb.database}')\n",
    "    .option('collection'    , f'{config.MongoDb.collection}')\n",
    "    .option('connection.uri', f'mongodb://{config.MongoDb.user}:{config.MongoDb.password}@{config.MongoDb.host}:{config.MongoDb.port}')\n",
    "    .mode('overwrite')\n",
    "\t.save()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
